# Story 1.5: Token Counter Module

## Status
Done

## Story
**As a** developer implementing workflow routing logic,
**I want** a token counter module that estimates repository token counts and makes routing decisions,
**so that** I can automatically determine whether to extract full or selective content based on the 200k token threshold.

## Acceptance Criteria
1. token_counter.py module created with all required functions
2. count_tokens() estimates token count from GitIngest stdout output
3. count_tokens_from_file() counts tokens in already-extracted files (for re-check)
4. should_extract_full() determines routing decision based on threshold
5. Character-based fallback estimation when GitIngest token count unavailable
6. Timeout protection for token counting (120 seconds default)
7. Unit tests cover token counting accuracy and edge cases

## Tasks / Subtasks

- [x] **Task 1: Create token_counter.py module with imports** (AC: 1)
  - [x] Create token_counter.py at project root
  - [x] Import subprocess for GitIngest execution
  - [x] Import re for parsing GitIngest output
  - [x] Import pathlib.Path for file operations
  - [x] Import exceptions (GitIngestError)

- [x] **Task 2: Implement count_tokens() function** (AC: 2, 5, 6)
  - [x] Accept url parameter (GitHub URL)
  - [x] Execute GitIngest with stdout output: `gitingest <url> -o -`
  - [x] Set timeout to 120 seconds (2 minutes)
  - [x] Parse "Estimated tokens: NNNN" from stdout
  - [x] Implement fallback: character count / 4 (if parsing fails)
  - [x] Raise GitIngestError for subprocess failures
  - [x] Raise TimeoutError for timeout expiration
  - [x] Return int (estimated token count)

- [x] **Task 3: Implement count_tokens_from_file() function** (AC: 3)
  - [x] Accept file_path parameter (Path or str)
  - [x] Read file content
  - [x] Estimate tokens: character count / 4
  - [x] Handle file not found errors gracefully
  - [x] Return int (estimated token count)

- [x] **Task 4: Implement should_extract_full() function** (AC: 4)
  - [x] Accept token_count parameter (int)
  - [x] Accept threshold parameter (int, default=200_000)
  - [x] Return True if token_count < threshold
  - [x] Return False if token_count >= threshold

- [x] **Task 5: Write unit tests for token_counter.py** (AC: 7)
  - [x] Test count_tokens() with mocked GitIngest output (< 200k)
  - [x] Test count_tokens() with mocked GitIngest output (>= 200k)
  - [x] Test count_tokens() fallback estimation (no "Estimated tokens" in output)
  - [x] Test count_tokens() timeout handling
  - [x] Test count_tokens() GitIngest error handling
  - [x] Test count_tokens_from_file() with real file
  - [x] Test count_tokens_from_file() file not found
  - [x] Test should_extract_full() with various thresholds
  - [x] Use pytest fixtures for temporary files
  - [x] Use unittest.mock.patch for subprocess mocking

## Dev Notes

### Previous Story Insights
Dependencies:
- Story 1.2 (exceptions) - Uses GitIngestError
- Story 1.4 (workflow) - Uses validate_github_url() for URL validation

### Architecture Overview
The token counter module is the decision engine for workflow routing. It wraps GitIngest CLI to estimate repository size and determines whether full or selective extraction is appropriate.

[Source: architecture.md#2.2 Token Counter Module, prd.md#FR1]

### Module Functions

**count_tokens(url: str) -> int**
```python
def count_tokens(url: str) -> int:
    """
    Count tokens in repository using GitIngest.

    Args:
        url: GitHub repository URL

    Returns:
        Estimated token count

    Raises:
        GitIngestError: If extraction fails
        TimeoutError: If counting takes too long

    Examples:
        >>> count_tokens("https://github.com/octocat/Hello-World")
        8500
    """
```
[Source: architecture.md#2.2]

**Implementation Strategy:**
```python
def count_tokens(url: str) -> int:
    # Strategy: Use GitIngest to stdout, estimate from output
    try:
        result = subprocess.run(
            ['gitingest', url, '-o', '-'],
            capture_output=True,
            text=True,
            timeout=120,
            check=True
        )
    except subprocess.TimeoutExpired:
        raise TimeoutError(f"Token counting timed out after 120 seconds")
    except subprocess.CalledProcessError as e:
        raise GitIngestError(f"GitIngest failed: {e.stderr}")

    # Try to parse "Estimated tokens: NNNN" from output
    match = re.search(r'Estimated tokens:\s*(\d+)', result.stdout)
    if match:
        return int(match.group(1))

    # Fallback: Character-based estimation (4 chars ≈ 1 token)
    return len(result.stdout) // 4
```
[Source: architecture.md#2.2]

**count_tokens_from_file(file_path: str) -> int**
```python
def count_tokens_from_file(file_path: str) -> int:
    """
    Count tokens in already-extracted file.
    Used for size re-check after selective extraction.

    Args:
        file_path: Path to extracted content file

    Returns:
        Estimated token count

    Raises:
        FileNotFoundError: If file doesn't exist

    Examples:
        >>> count_tokens_from_file("data/fastapi/docs-content.txt")
        89450
    """
```
[Source: architecture.md#2.2, prd.md#FR5]

**Implementation Strategy:**
```python
def count_tokens_from_file(file_path: str) -> int:
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")

    content = path.read_text(encoding='utf-8')
    # Same estimation: 4 chars ≈ 1 token
    return len(content) // 4
```

**should_extract_full(token_count: int, threshold: int = 200_000) -> bool**
```python
def should_extract_full(token_count: int, threshold: int = 200_000) -> bool:
    """
    Determine if full extraction is appropriate.

    Args:
        token_count: Repository token count
        threshold: Decision threshold (default 200k)

    Returns:
        True if should extract full, False if selective

    Examples:
        >>> should_extract_full(145000)
        True
        >>> should_extract_full(487523)
        False
    """
```
[Source: architecture.md#2.2, prd.md#US1]

**Implementation Strategy:**
```python
def should_extract_full(token_count: int, threshold: int = 200_000) -> bool:
    return token_count < threshold
```

### Design Decisions

**Why character-based fallback estimation:**
- GitIngest's "Estimated tokens" format may change
- Character count provides reasonable approximation (4 chars ≈ 1 token is industry standard)
- Ensures function always returns a value
- ±10% accuracy acceptable for routing decisions
[Source: architecture.md#2.2, prd.md#TR5]

**Why 120-second timeout:**
- Token counting requires full repository download
- Large repositories (> 100 MB) may take minutes
- 2 minutes provides reasonable balance
- User can retry for extremely large repositories
[Source: architecture.md#2.2, prd.md#TR4]

**Why separate count_tokens_from_file() function:**
- Re-check workflow needs to count already-extracted content
- Avoids redundant GitIngest calls
- File-based counting is faster (no network I/O)
- Critical for FR5: Token overflow prevention
[Source: architecture.md#2.2, prd.md#FR5]

**Why configurable threshold:**
- Default 200k matches Claude context window
- Parameterized for testing (can test with lower thresholds)
- Future-proof for different LLM context limits
- Allows user override in future enhancement
[Source: architecture.md#2.2]

**Token counting accuracy:**
```
Character-based estimation: ±10% accuracy
- English text: ~4-5 chars per token
- Code: ~3-4 chars per token (more symbols)
- Markdown: ~4-5 chars per token
Acceptable for routing decisions (threshold is 200k, margin is 20k)
```
[Source: prd.md#TR5, architecture.md#2.2]

### File Location

**Created in this story:**
- `token_counter.py` at project root

**Test file:**
- `tests/test_token_counter.py`

[Source: architecture.md#TR3 Project Structure]

### Error Handling Scenarios

**Network Errors:**
```python
# GitIngest fails to reach GitHub
except subprocess.CalledProcessError as e:
    if "could not resolve host" in e.stderr.lower():
        raise GitIngestError("Network error: Unable to reach GitHub")
    elif "repository not found" in e.stderr.lower():
        raise GitIngestError(f"Repository not found: {url}")
    else:
        raise GitIngestError(f"GitIngest error: {e.stderr}")
```
[Source: architecture.md#5.2, prd.md#TR4]

**Timeout Handling:**
```python
# Very large repository takes too long
except subprocess.TimeoutExpired:
    raise TimeoutError(
        f"Token counting timed out after 120 seconds. "
        f"Repository may be very large. Retry with 'extract-tree' command first, "
        f"then use 'extract-specific --type installation' for minimal content."
    )
```
[Source: architecture.md#5.2, prd.md#FR1]

**Why include retry strategy in message:**
- User knows exactly what to do next
- Points to specific commands available
- Suggests minimal extraction path (installation files)
- Reduces user frustration with actionable guidance

**Invalid URL Handling:**
```python
# Validate URL before counting (use workflow.validate_github_url)
from workflow import validate_github_url

def count_tokens(url: str) -> int:
    validate_github_url(url)  # Raises ValidationError if invalid
    # ... proceed with counting
```
[Source: architecture.md#2.2, prd.md#TR4]

### Performance Considerations

**Token Counting Time:**
- Small repos (< 50k tokens): < 15 seconds
- Medium repos (50-200k tokens): < 60 seconds
- Large repos (> 200k tokens): < 2 minutes
[Source: prd.md#6.4 Performance Benchmarks]

**Why no caching in Phase 1:**
- Adds complexity for minimal benefit
- Repositories change frequently
- Token counting is infrequent (once per analysis)
- Future enhancement: Cache token counts with TTL
[Source: architecture.md#7.2 Optimization Opportunities]

## Testing

### Testing Standards

**Test File Location:**
- `tests/test_token_counter.py`

**Testing Framework:**
- pytest framework
- unittest.mock.patch for subprocess mocking
- pytest.tmp_path fixture for file testing

**Test Coverage:**
```python
@patch('subprocess.run')
def test_count_tokens_under_threshold(mock_run):
    """Test counting repository under 200k threshold."""
    mock_run.return_value = Mock(
        stdout="Estimated tokens: 145000\n[content]",
        returncode=0
    )
    count = count_tokens("https://github.com/user/repo")
    assert count == 145000
    assert should_extract_full(count) == True

@patch('subprocess.run')
def test_count_tokens_over_threshold(mock_run):
    """Test counting repository over 200k threshold."""
    mock_run.return_value = Mock(
        stdout="Estimated tokens: 487523\n[content]",
        returncode=0
    )
    count = count_tokens("https://github.com/user/repo")
    assert count == 487523
    assert should_extract_full(count) == False

@patch('subprocess.run')
def test_count_tokens_fallback_estimation(mock_run):
    """Test fallback to character-based estimation."""
    mock_run.return_value = Mock(
        stdout="a" * 400_000,  # 100k tokens estimated
        returncode=0
    )
    count = count_tokens("https://github.com/user/repo")
    assert count == 100_000

@patch('subprocess.run')
def test_count_tokens_timeout(mock_run):
    """Test timeout handling for very large repos."""
    mock_run.side_effect = subprocess.TimeoutExpired(cmd=['gitingest'], timeout=120)
    with pytest.raises(TimeoutError):
        count_tokens("https://github.com/user/repo")

@patch('subprocess.run')
def test_count_tokens_network_error(mock_run):
    """Test network error handling."""
    mock_run.side_effect = subprocess.CalledProcessError(
        returncode=1,
        cmd=['gitingest'],
        stderr="could not resolve host"
    )
    with pytest.raises(GitIngestError):
        count_tokens("https://github.com/user/repo")

def test_count_tokens_from_file(tmp_path):
    """Test counting tokens from existing file."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("a" * 80_000, encoding='utf-8')  # 20k tokens
    count = count_tokens_from_file(str(test_file))
    assert count == 20_000

def test_count_tokens_from_file_not_found():
    """Test error handling for missing file."""
    with pytest.raises(FileNotFoundError):
        count_tokens_from_file("nonexistent.txt")

def test_should_extract_full_under_threshold():
    """Test routing decision for small repo."""
    assert should_extract_full(145_000) == True

def test_should_extract_full_over_threshold():
    """Test routing decision for large repo."""
    assert should_extract_full(487_523) == False

def test_should_extract_full_exactly_at_threshold():
    """Test routing decision at exact threshold."""
    assert should_extract_full(200_000) == False  # >= threshold = selective

def test_should_extract_full_custom_threshold():
    """Test routing with custom threshold."""
    assert should_extract_full(150_000, threshold=100_000) == False
```

**Coverage Target:** 90%+ (critical path for workflow routing)

### Success Criteria

**Functional Completeness:**
- [x] All 3 functions implemented
- [x] GitIngest subprocess wrapper working
- [x] Token estimation fallback implemented
- [x] Timeout protection working
- [x] File-based token counting working
- [x] Routing decision logic correct
- [x] All tests pass

**Code Quality:**
- [x] Comprehensive error handling
- [x] Clear type hints
- [x] Detailed docstrings with examples
- [x] Timeout values well-documented

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-29 | 1.0 | Initial story creation | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
- Token usage: ~76k / 200k tokens
- No issues encountered

### Completion Notes
Successfully implemented token_counter module with 3 functions for GitIngest integration:
- count_tokens(): Wraps GitIngest CLI to estimate repository token counts
  - Validates GitHub URL before calling GitIngest
  - Executes `gitingest <url> -o -` with 120-second timeout
  - Parses "Estimated tokens: NNNN" from stdout
  - Fallback: character count / 4 when parsing fails
  - Comprehensive error handling: network errors, 404s, auth failures, timeouts
  - Provides actionable error messages with suggested alternatives
- count_tokens_from_file(): Estimates tokens in already-extracted files
  - Used for re-check workflow after selective extraction
  - Character-based estimation: length / 4
  - Handles FileNotFoundError gracefully
- should_extract_full(): Routing decision based on 200k threshold
  - Returns True for full extraction (< threshold)
  - Returns False for selective extraction (>= threshold)
  - Configurable threshold parameter for testing/future use

Created comprehensive test suite with 30 tests organized into 4 test classes:
- TestCountTokens: 13 tests for GitIngest integration (mocked subprocess)
- TestCountTokensFromFile: 7 tests for file-based counting
- TestShouldExtractFull: 7 tests for routing logic
- TestIntegration: 3 tests for complete workflows

All functions have comprehensive docstrings, type hints, and error handling.

### File List
**Modified:**
- token_counter.py (32 statements, 100% coverage)

**Created:**
- tests/test_token_counter.py (176 statements, 100% coverage, 30 tests)

**Test Results:**
- 30 tests passed
- 0 tests failed
- Test execution time: 0.17s
- Coverage: 100% on token_counter.py

## QA Results
_Populated by QA Agent after implementation_